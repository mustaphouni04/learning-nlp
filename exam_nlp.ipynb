{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# EXAM\n",
        "# Import necessary libraries\n",
        "import pandas as pd \n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from typing import Union, Tuple, List, Optional, Dict\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_excel(\"ExercisesTest_filtered.xlsx\")\n",
        "\n",
        "# Scan the dataset, analyze it\n",
        "print(df.head())\n",
        "print(df.columns)\n",
        "print(df[\"summary\"].values[0:10])\n",
        "print(df[\"category\"].values[0:10])\n",
        "\n",
        "# Number of categories is 2\n",
        "print(df[\"category\"].unique())\n",
        "\n",
        "# Compute the lexical diversity of the dataset\n",
        "def return_lexical_diversity(df: pd.DataFrame) -> Tuple[float, List[str]]:\n",
        "    summaries = df[\"summary\"].values\n",
        "    sentences = [word_tokenize(str(text)) for text in summaries]\n",
        "    all_words = [word for sent in sentences for word in sent]\n",
        "    unique = set(all_words)\n",
        "\n",
        "    return len(all_words) / len(unique), all_words \n",
        "\n",
        "\n",
        "lexical_diversity, all_words = return_lexical_diversity(df)\n",
        "print(f\"The lexical diversity of the dataset is: {lexical_diversity}\")\n",
        "\n",
        "\"\"\"The dataset has a lexical diversity of 25.39 which suggests the vocabulary is not that rich\"\"\"\n",
        "\n",
        "# Use the stopwords module to filter bad words\n",
        "stopwords = stopwords.words('english')\n",
        "#print(stopwords)\n",
        "\n",
        "# Function used to plot frequency distribution and word cloud of clean words\n",
        "def word_analysis(all_words, stopwords: List[str]):\n",
        "    # Preprocess words prior to calculating frequency distribution and word cloud\n",
        "    specials = [\".\", \",\", \"...\", \"!\", \"-\", \"n't\", \"n's\", \"'s\"]\n",
        "    # First filtering stage\n",
        "    all_words = [word.lower() for word in all_words if word.lower() not in stopwords]\n",
        "    # Second filtering stage, normalize the words\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    all_words = [stemmer.stem(token) for token in all_words]\n",
        "\n",
        "    # Calculate word frequencies\n",
        "    fdist = FreqDist()\n",
        "    \n",
        "    for word in all_words:\n",
        "        fdist[word.lower()] += 1\n",
        "    \n",
        "    for sp in specials:\n",
        "        del fdist[sp]\n",
        "\n",
        "    # Sort them from most frequent to less frequent\n",
        "    fdist = {k:v for k,v in sorted(fdist.items(), key= lambda x:x[1], reverse=True)}\n",
        "   \n",
        "    # Get top 30\n",
        "    top_30 = list(fdist.items())[:29]\n",
        "    \n",
        "    # Plot bar chart\n",
        "    _, ax = plt.subplots()\n",
        "    words = [tup[0] for tup in top_30]\n",
        "    counts = [tup[1] for tup in top_30]\n",
        "    ax.bar(words,counts)\n",
        "    ax.set_ylabel('frequency')\n",
        "    ax.set_title('top 30 most frequent words')\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot word cloud\n",
        "    wordcloud = WordCloud(stopwords=stopwords, background_color='white', max_words=300).generate_from_frequencies(fdist)\n",
        "\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.clf()\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Perform word analysis\n",
        "word_analysis(all_words,stopwords)\n",
        "\n",
        "# Task -> Text categorization\n",
        "\"\"\"\n",
        "The goal is to see if a given text comes from the 'Home' category of reviews or 'Automotive' category.\n",
        "\"\"\"\n",
        "\n",
        "# Perform train test split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)\n",
        "\n",
        "\"\"\"\n",
        "Preprocess the text, we use SnowballStemmer and stopwords to clean all the junk from the dataset.\n",
        "\"\"\"\n",
        "def preprocess_text(df: pd.DataFrame) -> List[str]:\n",
        "    # Get summaries and categories\n",
        "    summaries = df[\"summary\"].values\n",
        "    categories = df[\"category\"].values\n",
        "    # Turn categories into numbers\n",
        "    categories = [0 if cat == 'Home' else 'Automotive' for cat in categories]\n",
        "    # Tokenize\n",
        "    sentences = [word_tokenize(str(text)) for text in summaries]\n",
        "    filtered_sentences = []\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    \n",
        "    # Filtering all sentences\n",
        "    specials = [\".\", \",\", \"...\", \"!\", \"-\", \"n't\", \"n's\", \"'s\"]\n",
        "    for sentence in sentences:\n",
        "        tokens = [tok for tok in sentence if tok not in specials and tok not in stopwords] \n",
        "        tokens = [stemmer.stem(token) for token in tokens]\n",
        "        sentence = \" \".join(tokens)\n",
        "        filtered_sentences.append(sentence)\n",
        "\n",
        "    return filtered_sentences, categories\n",
        "        \n",
        "# Preprocess the sentences\n",
        "train_sentences, train_categories = preprocess_text(train_df)\n",
        "test_sentences, test_categories = preprocess_text(test_df)\n",
        "\n",
        "# Use nlp module to calculate word embeddings\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Get vectorial representations to fit then for model\n",
        "X_train = [nlp(str(sentence)).vector for sentence in tqdm(train_sentences)]\n",
        "    \n",
        "X_train_arr = np.zeros((len(train_sentences),300)) # Embedding dim is 300\n",
        "\n",
        "for idx,i in enumerate(X_train):\n",
        "    X_train_arr[idx] = X_train[idx]\n",
        "\n",
        "X_test = [nlp(str(sentence)).vector for sentence in tqdm(test_sentences)]\n",
        "\n",
        "X_test_arr = np.zeros((len(test_sentences),300)) # Embedding dim is 300\n",
        "\n",
        "for idx,i in enumerate(X_test):\n",
        "    X_test_arr[idx] = X_test[idx]\n",
        "\n",
        "print(X_test_arr.shape)\n",
        "# Get vectors for the categories as well\n",
        "y_train = np.array(train_categories)\n",
        "y_test = np.array(test_categories)\n",
        "\n",
        "clf = LogisticRegression(random_state=0).fit(X_train_arr, y_train)\n",
        "\n",
        "# Used to evaluate the accuracy of classification\n",
        "def evaluate(clf, test_array, target):\n",
        "    score = 0\n",
        "    total = test_array.shape[0]\n",
        "    for idx, vector in enumerate(test_array):\n",
        "        pred = clf.predict(vector.reshape(1,-1)) # Predict the class\n",
        "        if str(pred[0]) == str(target[idx]):\n",
        "            score += 1\n",
        "    accuracy = score / total\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "accuracy = evaluate(clf, X_test_arr, y_test)\n",
        "\n",
        "print(\"The accuracy for Word2Vec is: \", accuracy)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Summary of the task: The NLP based task performed is a text classification task. \n",
        "We compute word embeddings using Word2Vec to get dense representations of summaries that capture meaning.\n",
        "In this case, the accuracy is not that high ~0.65, what could we do to improve it?\n",
        "First off:\n",
        "- We use SnowballStemmer which is kind of aggressive and can suppress meaning from the words.\n",
        "- Logisitic Regression doesn't take order into account.\n",
        "\n",
        "Steps to improve:\n",
        "- Use a sequence-based model like RNNs, GRU or Transformer.\n",
        "- Preprocess the text in a different way.\n",
        "- Use a tokenizer, like the one from GPT-2.\n",
        "\n",
        "Overall, the task is to predict the kind of review (summary) from the texts.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
